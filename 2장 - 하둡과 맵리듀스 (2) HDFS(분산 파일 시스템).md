---
typora-copy-images-to: ..\..\..\..\..\..\..\data
---

## 2장 - 하둡과 맵리듀스 (2) HDFS(분산 파일 시스템)

​     

#### ※ 이곳에 적은 내용은 데이터 분석 전문가 가이드에서 설명하는 하둡 1버전입니다.

​     

### Ⅰ  개요

- 하둡은 구글에서 발표한 GFS(Google File System)를 발전한 시스템입니다. GFS와 세부적인 용어만 다르고 기본 개념은 동일하므로 하둡을 공부하면 GFS도 쉽게 이해할 수 있습니다


![hadoop](C:\data\hadoop-1538138132432.png)

HDFS는 대용량의 파일을 저장하는 시스템입니다.





### Ⅱ  HDFS의 구조



- HDFS는 블록 구조를 사용해 파일을 저장합니다. 
- 블록 구조 : 원 데이터 그대로가 아니라, 64MB의 블록으로 데이터를 나누고 블록을 복사해 서버의 여러 곳에 저장하는 시스템입니다. 
- 블록 단위로 입출력을 실시하면 블록을 찾는 시간과 블록의 메타데이터의 크기가 줄어듭니다. 동일한 블록을 3개로 만들므로 블록이 저장된 서버에 접속할 수 없어도 다른 서버에 접속해 데이터 처리가 가능합니다. 

​    

#### 1) 네임노드(Namenode) = 마스터 서버(HDFS를 주관하는 서버입니다)

> ① 메타데이터 관리 : 어떤 데이터가 어떤 데이터노드에 있는지 정리한 목록을 만들어 네임노드에 저장합니다
>
> ② 모니터링 : 어떤 데이터노드에 접근 가능하고 불가능한지 네임노드가 알아야 합니다 
>
> →  데이터노드가 살아있으면(접근 가능하면) 3초마다 하트비트라는 전송 신호를 네임노드로 보냅니다
>
> ③ 블록 관리 : 블록을 복사하거나 이동하거나 삭제하는 역할을 맡습니다
>
> ④ 클라이언트 요청 접수 : 클라이언트(개발자)가 HDFS에 파일을 저장하거나 조회하려면 네임노드에서 허가를 받아야 합니다. 이 부분은 Ⅲ에서 자세히 보겠습니다.

​     

#### ② 데이터 노드(datanode) : 네임노드의 지시에 따라 데이터가 실제로 저장되는 공간입니다

​     

#### ③ 보조 네임노드(secondary namenode) : 네임노드가 문제가 생길 경우를 대비한 백업 공간입니다. 네임노드에 있는 메타데이터들을 주기적으로 백업하는데, 이 작업을 체크포인트라고 합니다.

​     



### Ⅲ HDFS 작동 방식

​     

#### 1) HDFS에 파일을 저장하는 경우

​     

1. 클라이언트(개발자 or 개발자가 만든 프로그램)는 HDFS에 파일을 저장하는 Output 스트림을 요청하고 받습니다.

2. <u>네임노드는 Output 스트림이 유효한지 확인합니다.</u>

3. Output 스트림이 유효하면 네임노드는 데이터노드의 목록을 반환합니다

4. HDFS에 저장할 파일을 패킷으로 분할해 데이터노드에 전송합니다.

5. 설정해 놓은 복제본 수에 맞춰 데이터노드 간 패킷을 교환합니다.(데이터 복사)

6. 클라이언트의 파일이 모두 전송되면 네임노드에 저장 결과를 보냅니다.

7. 네임노드가 복제본 수에 맞춰 저장이 잘 되었다고 판단하면, Output 스트림을 닫습니다.

   ​     

#### 2) HDFS에 있는 파일을 조회하는 경우

​     

1. 클라이언트는 HDFS의 파일을 조회하는 Input 스트림을 요청하고 받습니다.

2. Input 스트림을 받으면서 동시에 블록이 있는 위치 정보도 받습니다.

3. 데이터 노드에 있는 블록을 읽기 위해 블록 리더기(Reader)를 만듭니다.

   (RemoteBlockReader가 BlockReaderLocal에 비해 먼 곳에 블록이 위치하므로 속도가 느립니다)

4. 모든 블록을 조회하면, Input 스트림과 블록 리더기를 닫습니다.




cf) Output 스트림과 Input 스트림의 기준은 클라이언트입니다.

HDFS에 업로드를 하면 내 컴퓨터에서 자료가 나가며(Output Stream), HDFS에 올라간 파일을 조회(다운로드)하면 내 컴퓨터에 자료가 들어온다고(Input Stream) 이해하면 됩니다.



cf2) sqoop 에코시스템은 HDFS를 기준으로 이해하면 됩니다.

HDFS에 데이터를 저장하는 경우 Import, HDFS에 있는 데이터를 조회하면 Export 명령어를 사용합니다.





##### 참고 문헌

정재화, 「시작하세요! 하둡 프로그래밍(개정2판)」, 위키북스, 2016.

​     

B강사님의 강의노트

​     



-- 2018.09.28